{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_FILE=\"SpamData/01_Processing/practice_email.txt\"\n",
    "SPAM_PATH_1=\"SpamData/01_Processing/spam_assassin_corpus/spam_1\"\n",
    "SPAM_PATH_2=\"SpamData/01_Processing/spam_assassin_corpus/spam_2\"\n",
    "EASY_NONSPAM_PATH_1=\"SpamData/01_Processing/spam_assassin_corpus/easy_ham_1\"\n",
    "EASY_NONSPAM_PATH_2=\"SpamData/01_Processing/spam_assassin_corpus/easy_ham_2\"\n",
    "DATA_JSON=\"SpamData/01_Processing/email-text-data.json\"\n",
    "\n",
    "TOKEN_SPAM_PROB_FILE = 'SpamData/03_Testing/prob-spam.txt'\n",
    "TOKEN_HAM_PROB_FILE = 'SpamData/03_Testing/prob-nonspam.txt'\n",
    "TOKEN_ALL_PROB_FILE = 'SpamData/03_Testing/prob-all-tokens.txt'\n",
    "\n",
    "TEST_FEATURE_MATRIX = 'SpamData/03_Testing/test-features.txt'\n",
    "TEST_TARGET_FILE = 'SpamData/03_Testing/test-target.txt'\n",
    "\n",
    "VOCAB_SIZE = 2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Email Body Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_body_generator(path):\n",
    "    for root,directories,file_names in walk(path):\n",
    "        for file_name in file_names:\n",
    "            file_path=join(root,file_name)\n",
    "            stream=open(file_path,encoding='latin-1')\n",
    "            is_body=False\n",
    "            lines=[]\n",
    "            for line in stream:\n",
    "                if is_body:\n",
    "                    lines.append(line)\n",
    "                elif line=='\\n':\n",
    "                    is_body=True\n",
    "            stream.close()\n",
    "            email_body='\\n'.join(lines)\n",
    "            yield file_name,email_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_generator(path,classification):\n",
    "    rows=[]\n",
    "    row_names=[]\n",
    "    for file_name,email_body in email_body_generator(path):\n",
    "        rows.append({\"CATEGORY\":classification,\"MESSAGE\":email_body})\n",
    "        row_names.append(file_name)\n",
    "    return pd.DataFrame(rows,index=row_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Dataset (Spam,Ham,All mails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_emails=df_generator(SPAM_PATH_1,1)\n",
    "spam_emails=spam_emails.append(df_generator(SPAM_PATH_2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails=df_generator(EASY_NONSPAM_PATH_1,0)\n",
    "ham_emails=ham_emails.append(df_generator(EASY_NONSPAM_PATH_2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([spam_emails,ham_emails])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Filename to Document ID's and setting as Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids=range(0,len(data.index))\n",
    "data['DOC_ID']=document_ids\n",
    "data.set_index('DOC_ID',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing messages with 0 length messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(['cmds'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(DATA_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Spam and Ham Data Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.CATEGORY.value_counts()\n",
    "amount_of_spam=data.CATEGORY.value_counts()[0]\n",
    "amount_of_ham=data.CATEGORY.value_counts()[1]\n",
    "category_names=['SPAM','LEGIT']\n",
    "sizes=[amount_of_spam,amount_of_ham]\n",
    "custom_colors=['#55efc4','#ffeaa7']\n",
    "plt.figure(figsize=(2,2),dpi=227)\n",
    "plt.pie(sizes,labels=category_names,textprops={'fontsize':8},startangle=90,autopct='%1.0f%%',colors=custom_colors,explode=[0,0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK for removing StopWords and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shantanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shantanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function To Clean Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_msg(message,stemmer=PorterStemmer(),stop_words=set(stopwords.words('english'))):\n",
    "    soup=BeautifulSoup(message,'html.parser')\n",
    "    cleand_text=soup.get_text()\n",
    "    filtered_words=[]\n",
    "    words=word_tokenize(cleand_text.lower())\n",
    "    for word in words:\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "    return filtered_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:357: UserWarning: \"http://www.post-gazette.com/columnists/20020905brian5\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "nested_list=data.MESSAGE.apply(clean_msg)\n",
    "flat_list=[item for sublist in nested_list for item in sublist]\n",
    "unique_words=pd.Series(flat_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_doc_ids=data[data.CATEGORY==1].index\n",
    "ham_doc_ids=data[data.CATEGORY==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list_ham=nested_list.loc[ham_doc_ids]\n",
    "nested_list_spam=nested_list.loc[spam_doc_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_spam=[item for sublist in nested_list_spam for item in sublist]\n",
    "flat_list_ham=[item for sublist in nested_list_ham for item in sublist]\n",
    "normal_words=pd.Series(flat_list_ham).value_counts()\n",
    "spammy_words=pd.Series(flat_list_spam).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words=unique_words[:VOCAB_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids=list(range(0,VOCAB_SIZE))\n",
    "vocab=pd.DataFrame({'VOCAB_WORD':frequent_words.index.values},index=word_ids)\n",
    "vocab.index.name='WORD_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Column Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_column_df=pd.DataFrame.from_records(nested_list.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(word_column_df,data.CATEGORY,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index.name=X_test.index.name='DOC_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A Word Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=pd.Index(vocab.VOCAB_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df,index_words,lables):\n",
    "    nr_rows=df.shape[0]\n",
    "    nr_cols=df.shape[1]\n",
    "    word_set=set(index_words)\n",
    "    dict_list=[]\n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            word=df.iat[i,j]\n",
    "            if word in word_set:\n",
    "                doc_id=df.index[i]\n",
    "                word_id=index_words.get_loc(word)\n",
    "                category=lables.at[doc_id]\n",
    "                item={'LABEL':category,'DOC_ID':doc_id,'OCCURENCE':1,'WORD_ID':word_id}\n",
    "                dict_list.append(item)\n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_train_matrix=make_sparse_matrix(X_train,word_index,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_group=sparse_train_matrix.groupby(['DOC_ID','WORD_ID','LABEL']).sum()\n",
    "trained_group=trained_group.reset_index()\n",
    "sparse_train_data=trained_group.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sparse_test_matrix=make_sparse_matrix(X_test,word_index,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_group=sparse_test_matrix.groupby(['DOC_ID','WORD_ID','LABEL']).sum()\n",
    "test_group=test_group.reset_index()\n",
    "sparse_test_data=test_group.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A Full Matrix From Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_matrix(sparse_matrix,nr_words,doc_idx=0,word_idx=1,cat_idx=2,freq_idx=3):\n",
    "    index_names=np.unique(sparse_matrix[:,0])\n",
    "    column_names=['DOC_ID']+['CATEGORY']+list(range(0,2500))\n",
    "    full_matrix=pd.DataFrame(index=index_names,columns=column_names)\n",
    "    full_matrix.fillna(value=0,inplace=True)\n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr=sparse_matrix[i][doc_idx]\n",
    "        word_id=sparse_matrix[i][word_idx]\n",
    "        label=sparse_matrix[i][cat_idx]\n",
    "        occurence=sparse_matrix[i][freq_idx]\n",
    "        full_matrix.at[doc_nr,'DOC_ID']=doc_nr\n",
    "        full_matrix.at[doc_nr,'CATEGORY']=label\n",
    "        full_matrix.at[doc_nr,word_id]=occurence\n",
    "    full_matrix.set_index(['DOC_ID'],inplace=True)\n",
    "    \n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "full_train_data=make_full_matrix(sparse_train_data,2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_data=make_full_matrix(sparse_test_data,2500)\n",
    "X_test=full_test_data.loc[:,full_test_data.columns!=\"CATEGORY\"].to_numpy()\n",
    "y_test=full_test_data.CATEGORY.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Calcuation's For Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_features=full_train_data.loc[:,full_train_data.columns!='CATEGORY']\n",
    "email_lengths=full_features.sum(axis=1)\n",
    "total_wc=email_lengths.sum()\n",
    "spam_wc=email_lengths[full_train_data.CATEGORY==1].sum()\n",
    "ham_wc=email_lengths[full_train_data.CATEGORY==0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_spam_tokens=full_features[full_train_data.CATEGORY==1]\n",
    "summed_span_tokens=trained_spam_tokens.sum(axis=0)+1\n",
    "trained_ham_tokens=full_features[full_train_data.CATEGORY==0]\n",
    "summed_han_tokens=trained_ham_tokens.sum(axis=0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_spam_tokens=summed_span_tokens/spam_wc\n",
    "prob_ham_tokens=summed_han_tokens/ham_wc\n",
    "prob_tokens_all=full_features.sum(axis=0)/total_wc\n",
    "\n",
    "\n",
    "prob_token_spam=prob_spam_tokens.to_numpy()\n",
    "prob_token_ham=prob_ham_tokens.to_numpy()\n",
    "prob_token_all=prob_tokens_all.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3109118086696562\n"
     ]
    }
   ],
   "source": [
    "prob_spam=full_train_data.CATEGORY.sum()/full_train_data.CATEGORY.size\n",
    "print(prob_spam)\n",
    "PROB_SPAM=0.3116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_log_spam=X_test.dot(np.log(prob_token_spam)-np.log(prob_all_tokens))+np.log(PROB_SPAM)\n",
    "joint_log_ham=X_test.dot(np.log(prob_token_ham)-np.log(prob_all_tokens))+np.log(1-PROB_SPAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=joint_log_spam>joint_log_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:5]*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
